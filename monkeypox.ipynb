{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d727af-3fdf-4486-ac98-406dd67d9e00",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd48e14a-7f0c-4a7d-8099-fcab921b9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0df3f-180c-4bf1-a9ef-29125401e01f",
   "metadata": {},
   "source": [
    "Creating a img augmentation generator- Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7738e92e-babf-4d58-a2ba-f659343da81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e0de1-a916-4d07-9aeb-6f6ae054b1f1",
   "metadata": {},
   "source": [
    "Creates batches of images from directory (along with labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd52f936-5142-4250-be90-173e32382dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3360 images belonging to 2 classes.\n",
      "Found 49 images belonging to 2 classes.\n",
      "Found 185 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\suhan\\Desktop\\bin\\train\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\suhan\\Desktop\\bin\\valid\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\suhan\\Desktop\\bin\\test\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1125eb-6aa4-435a-a2d0-ecb16bf0ffc6",
   "metadata": {},
   "source": [
    "Performing 3-fold cross validation (since dataset is smaller, this improves model's generalisability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105346ca-bcfe-4928-9c17-b8f8b9f2d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66a2da7-1f05-45a6-96ee-8c675d7c69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = r\"C:\\Users\\suhan\\Desktop\\bin\"\n",
    "FOLDS = 3\n",
    "TARGET_PATH = r\"C:\\Users\\suhan\\Desktop\\bin\\folds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e746eb0-47e7-4a69-b77b-fba53c8a5c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds created and images copied.\n"
     ]
    }
   ],
   "source": [
    "# create fold funct\n",
    "def create_folds(base_path, num_folds=FOLDS):\n",
    "    train_path = os.path.join(base_path, \"train\")\n",
    "    folds_dir = os.path.join(TARGET_PATH)\n",
    "    if not os.path.exists(folds_dir):\n",
    "        os.makedirs(folds_dir)\n",
    "\n",
    "    all_images = []\n",
    "    for class_dir in os.listdir(train_path):\n",
    "        class_dir_path = os.path.join(train_path, class_dir)\n",
    "        if os.path.isdir(class_dir_path):\n",
    "            for img in os.listdir(class_dir_path):\n",
    "                all_images.append((os.path.join(class_dir_path, img), class_dir))\n",
    "\n",
    "    np.random.shuffle(all_images)\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_dirs = []\n",
    "    for fold in range(num_folds):\n",
    "        fold_train_dir = os.path.join(folds_dir, f\"fold_{fold + 1}_train\")\n",
    "        fold_valid_dir = os.path.join(folds_dir, f\"fold_{fold + 1}_valid\")\n",
    "\n",
    "        for d in [fold_train_dir, fold_valid_dir]:\n",
    "            if not os.path.exists(d):\n",
    "                os.makedirs(d)\n",
    "                for class_dir in os.listdir(train_path):\n",
    "                    os.makedirs(os.path.join(d, class_dir), exist_ok=True)\n",
    "\n",
    "        fold_dirs.append((fold_train_dir, fold_valid_dir))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(all_images)):\n",
    "        train_images = [all_images[i] for i in train_idx]\n",
    "        valid_images = [all_images[i] for i in valid_idx]\n",
    "\n",
    "        train_dir, valid_dir = fold_dirs[fold]\n",
    "\n",
    "        for image_path, class_dir in train_images:\n",
    "            dst_dir = os.path.join(train_dir, class_dir)\n",
    "            shutil.copy(image_path, dst_dir)\n",
    "\n",
    "        for image_path, class_dir in valid_images:\n",
    "            dst_dir = os.path.join(valid_dir, class_dir)\n",
    "            shutil.copy(image_path, dst_dir)\n",
    "\n",
    "    print(\"Folds created and images copied.\")\n",
    "\n",
    "\n",
    "#calling fold funct\n",
    "create_folds(BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd3d4d-2f7f-45a8-bfca-599c7b3bb77d",
   "metadata": {},
   "source": [
    "Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432740fb-b6a4-4f2a-afca-86bf531a4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_model():\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf056b8-2199-43e8-bd75-e13e53e6402c",
   "metadata": {},
   "source": [
    "Running the model over the generated folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65c0f5-63de-4c73-9d90-644c271296b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1\n",
      "Found 3347 images belonging to 2 classes.\n",
      "Found 2935 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.7146 - loss: 0.5550 - val_accuracy: 0.5055 - val_loss: 0.6928\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/104\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 902ms/step - accuracy: 0.8125 - loss: 0.4561"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8125 - loss: 0.4561 - val_accuracy: 0.5217 - val_loss: 0.6928\n",
      "Epoch 3/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.8812 - loss: 0.2912 - val_accuracy: 0.5055 - val_loss: 0.6958\n",
      "Epoch 4/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0997 - val_accuracy: 0.4783 - val_loss: 0.6954\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 273ms/step - accuracy: 0.5107 - loss: 0.6952\n",
      "Fold 1 - Validation Accuracy: 0.5054945349693298\n",
      "Training Fold 2\n",
      "Found 3352 images belonging to 2 classes.\n",
      "Found 2938 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m 56/104\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.6469 - loss: 0.6159"
     ]
    }
   ],
   "source": [
    "def cross_validate(num_folds=FOLDS):\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "        print(f\"Training Fold {fold + 1}\")\n",
    "        \n",
    "        # data genes for this fold\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        \n",
    "        val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "        train_dir = os.path.join(TARGET_PATH, f\"fold_{fold + 1}_train\")\n",
    "        val_dir = os.path.join(TARGET_PATH, f\"fold_{fold + 1}_valid\")\n",
    "        \n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=(150, 150),\n",
    "            batch_size=32,\n",
    "            class_mode='binary'\n",
    "        )\n",
    "        \n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            val_dir,\n",
    "            target_size=(150, 150),\n",
    "            batch_size=32,\n",
    "            class_mode='binary'\n",
    "        )\n",
    "        \n",
    "        model = build_model()\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "            epochs=10,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=val_generator.samples // val_generator.batch_size,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc = model.evaluate(val_generator, steps=val_generator.samples // val_generator.batch_size)\n",
    "        fold_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} - Validation Accuracy: {val_acc}\")\n",
    "\n",
    "    print(f\"Mean Validation Accuracy across folds: {np.mean(fold_accuracies)}\")\n",
    "\n",
    "#run\n",
    "cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79af80-3b70-4125-935e-52f11b6adfc2",
   "metadata": {},
   "source": [
    "Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c80d0-d76b-42f4-a364-9932e8d6d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate predictions on the test set\n",
    "test_generator.reset()  # Reset the test generator to ensure correct predictions\n",
    "predictions = final_model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "# True labels\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())  # List of class names\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
